{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Python modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime, timedelta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataframe Creation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read target csv to a Dataframe, ignore all bad lines. Python will tell me which lines are ignored for manual cleanup. Only seems to happen when boards were rebooted - no data loss, easy to fix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = pd.read_csv('../../SRAM3/1/half/nvRAM_data_02-06-2021-1720.csv', error_bad_lines=False)\r\n",
    "log_length = data.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identifying Runs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a list of every instance of M,A,WRCHCKBRD. This log line indicates the start of every new experiment run. Should probably make this a user input to this Python so that different experiment setups are easily accounted for.\n",
    "\n",
    "Create a list of every instance of M,A,INIT. Indicates start of board power cycle.\n",
    "\n",
    "This method is the easiest, but will label all INIT cycles as a separate experimental run. No loss of data occurs so this is ok."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "wrchckbrd_locs = data.loc[(data['Board'] == 'M') & (data['Mikroe_socket'] == 'A') & (data['Status'] == 'WRCHCKBRD')].index.tolist()\r\n",
    "wrchckbrd_index = [(i, 'WRCHCKBRD') for i in wrchckbrd_locs]\r\n",
    "init_locs = data.loc[(data['Board'] == 'M') & (data['Mikroe_socket'] == 'A') & (data['Status'] == 'INIT')].index.tolist()\r\n",
    "init_index = [(i, 'INIT') for i in init_locs]"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine both lists, and append new list with final log line number."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "run_index = wrchckbrd_index + init_index\r\n",
    "run_index = sorted(run_index)\r\n",
    "run_index.append((log_length, 'END'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each element h in c, if the status c[h][1] is WRCHCKBRD, label everything between its index c[h][0] and the next index c[h+1][0] in c with the same run ID. If the status is INIT, no labels. This ensures that only actual test sequences are associated as runs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "run_id = 1\r\n",
    "run_col = ['']*log_length\r\n",
    "for h in range(len(run_index)-1):\r\n",
    "    if run_index[h][1] == 'WRCHCKBRD':\r\n",
    "        start = run_index[h][0]\r\n",
    "        end = run_index[h+1][0]\r\n",
    "        run_col[start:end] = [run_id]*(end-start)\r\n",
    "        run_id+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data.insert(loc=0, column='Run_ID', value=run_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neutron Logs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get first/last log timestamps into datetime objects"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "exp_tformat = '%Y-%m-%d_%H-%M-%S-%f'\r\n",
    "exp_firststamp = datetime.strptime(data.iloc[0]['Timestamp'], exp_tformat)\r\n",
    "exp_laststamp = datetime.strptime(data.iloc[log_length-1]['Timestamp'], exp_tformat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get all neutron logs for the days that the experiment was running."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "beamlog_fname_tformat = '%Y-%m-%d'\r\n",
    "beamlog_firstday = datetime.strftime(exp_firststamp, beamlog_fname_tformat)\r\n",
    "beamlog_lastday = datetime.strftime(exp_laststamp, beamlog_fname_tformat)\r\n",
    "beamlog_day_dif = int(datetime.strftime(exp_laststamp, \"%d\")) - int(datetime.strftime(exp_firststamp, \"%d\"))\r\n",
    "beamlog = pd.read_csv(f\"../../../neutrons/countlog-{beamlog_firstday}.txt\", delim_whitespace=True, header=None, skiprows=1)\r\n",
    "beamlog_nextday = beamlog_firstday\r\n",
    "for i in range(0, beamlog_day_dif):\r\n",
    "    beamlog_nextday = datetime.strftime((exp_firststamp + timedelta(days=1)), beamlog_fname_tformat)\r\n",
    "    beamlog_next = pd.read_csv(f\"../../../neutrons/countlog-{beamlog_nextday}.txt\", delim_whitespace=True, header=None, skiprows=1)\r\n",
    "    beamlog_frames = [beamlog, beamlog_next]\r\n",
    "    beamlog = pd.concat(beamlog_frames, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remake neutron log column names"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "beamlog_new_colnames = [\"Date\", \"HMS_time\", \"Millisecs\", \"Count1\", \"Count2\", \"Count3\", \"Count4\", \"protonCharge\", \"Beam_current\"]\r\n",
    "beamlog.columns = beamlog_new_colnames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create list of datetime objects of all timestamps in beamlog"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "beamlog_tstamps_concat = (beamlog['Date'] + beamlog['HMS_time'] + beamlog['Millisecs'].apply(str)).tolist()\r\n",
    "b_tstamps_concat_cln = []\r\n",
    "for i in beamlog_tstamps_concat:\r\n",
    "    if len(i) > 23:\r\n",
    "        i = i[0:26]\r\n",
    "    b_tstamps_concat_cln.append(i)\r\n",
    "\r\n",
    "b_tstamps_concat_cln = [datetime.strptime(i, '%d/%m/%Y%H:%M:%S0.%f') for i in b_tstamps_concat_cln]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to check if beam was on between two experiment log times. Returns 1 if beam was on, 0 if not."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def beam_on(firsttime, lasttime):\r\n",
    "    beamlog_stamp_tformat = '%d/%m/%Y %H:%M:%S'\r\n",
    "    firsttime_obj = datetime.strptime(firsttime, exp_tformat)\r\n",
    "    lasttime_obj = datetime.strptime(lasttime, exp_tformat) \r\n",
    "    nearest_beamtime_to_firsttime = min([i for i in b_tstamps_concat_cln if i <= firsttime_obj], key=lambda x: abs(x - firsttime_obj))\r\n",
    "    nearest_beamtime_to_lasttime = min([i for i in b_tstamps_concat_cln if i >= lasttime_obj], key=lambda x: abs(x - lasttime_obj))\r\n",
    "    firstrow = beamlog.loc[(beamlog['Date'] == datetime.strftime(nearest_beamtime_to_firsttime, '%d/%m/%Y')) & (beamlog['HMS_time'] == datetime.strftime(nearest_beamtime_to_firsttime, '%H:%M:%S'))]\r\n",
    "    lastrow = beamlog.loc[(beamlog['Date'] == datetime.strftime(nearest_beamtime_to_lasttime, '%d/%m/%Y')) & (beamlog['HMS_time'] == datetime.strftime(nearest_beamtime_to_lasttime, '%H:%M:%S'))]\r\n",
    "    count4_dif = lastrow.iloc[0]['Count4'] - firstrow.iloc[0]['Count4']\r\n",
    "    num_rows = lastrow.index.astype(int)[0] - firstrow.index.astype(int)[0]\r\n",
    "    cps = count4_dif / num_rows\r\n",
    "    if cps > 1:\r\n",
    "        return 1\r\n",
    "    return 0\r\n",
    "\r\n",
    "#beam_on('2021-06-02_17-35-03-568', '2021-06-02_17-35-24-588')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "before_delay = data.loc[(data['Run_ID'] == 1)].loc[(data['Mikroe_socket'] == 'D') & (data['Status'] == 'STORE_OK')].index\r\n",
    "after_delay_index = data.loc[(data['Run_ID']) == 1].loc[(data['Mikroe_socket'] == 'A') & (data['Status'] == 'VERIF')].index[0] + 1\r\n",
    "\r\n",
    "data.iloc[after_delay_index]['Timestamp']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-06-02_17-20-16-004319'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating run objects"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Run:\r\n",
    "    def __init__(self, name):\r\n",
    "        self.name = name\r\n",
    "        self.num = [int(s) for s in name.split('_') if s.isdigit()][0]\r\n",
    "        self.df = data.loc[(data['Run_ID'] == self.num)]\r\n",
    "\r\n",
    "    def delay(self):\r\n",
    "        before_delay_i = self.df.loc[(self.df['Mikroe_socket'] == 'D') & (self.df['Status'] == 'STORE_OK')].index[0]\r\n",
    "        after_delay_i  = self.df.loc[(self.df['Mikroe_socket']) == 'A' & (self.df['Status'] == 'VERIF')].index[0] + 1\r\n",
    "        before_delay_t = self.df.iloc[before_delay_i]['Timestamp']\r\n",
    "        after_delay_t  = self.df.iloc[after_delay_i]['Timestamp']\r\n",
    "        before_delay_dt = datetime.strptime(before_delay_t, exp_tformat)\r\n",
    "        after_delay_dt  = datetime.strptime(after_delay_t, exp_tformat)\r\n",
    "        # get delay between dt objects, if less than a second delay = 0.1s, if less than 10 seconds, delay = 1s etc\r\n",
    "\r\n",
    "    def error_tot(self):\r\n",
    "        self.error_num = self.df['Status'].value_counts()['SDC']\r\n",
    "        return(self.error_num)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_names = []\r\n",
    "for i in range(1, run_col[-1]+1):\r\n",
    "    run_names.append(f\"run_{i}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "runs = {}\r\n",
    "for i in run_names:\r\n",
    "    runs[i] = Run(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "runs[\"run_79\"].error_tot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How many bits tend to get flipped?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get occurrences of each type of error"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_see = data.loc[(data['Status'] == 'SDC')].index\r\n",
    "see = data['SDC_val']\r\n",
    "see_types_hex = see.value_counts().index.tolist()\r\n",
    "see_freqs = see.value_counts().tolist()\r\n",
    "see_bins = list(zip(see_types_hex, see_freqs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use this for Hamming distance between errors and golden value:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xortest1 = 85\r\n",
    "xortest2 = 170\r\n",
    "\r\n",
    "bin(xortest1 ^ xortest2).count('1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Where do errors tend to happen?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "see_loc = data['SDC_loc']\r\n",
    "see_locs_hex = see_loc.value_counts().index.tolist()\r\n",
    "see_loc_freqs = see_loc.value_counts().tolist()\r\n",
    "see_locs_bins = list(zip(see_locs_hex, see_loc_freqs))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68e83ffa79d15a4f3f363f7e2d730fba250349155b7434a2a310aead7dfd6146"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}